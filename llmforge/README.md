# ğŸ”¥ LLMForge

**Production LLM Fine-Tuning & Deployment Platform**

LLMForge is a complete MLOps platform for fine-tuning and deploying custom Large Language Models (LLMs). Train models with QLoRA on GCP, evaluate with automated benchmarks, and deploy with vLLM inference engine.

![LLMForge Architecture](https://img.shields.io/badge/Platform-LLMForge-blue)
![Python](https://img.shields.io/badge/Python-3.11-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸ“‹ Features

### Training
- âœ… **QLoRA Fine-Tuning**: 4-bit NF4 quantization, 75% memory reduction
- âœ… **Model Support**: Llama 3.1 8B/70B, Mistral 7B v0.3
- âœ… **MLflow Integration**: Automatic experiment tracking
- âœ… **Hyperparameter Validation**: Prevent common mistakes
- âœ… **GCP Native**: Train on Vertex AI with A100 GPUs

### Deployment
- âœ… **vLLM Inference**: 2-3x higher throughput with PagedAttention
- âœ… **OpenAI-Compatible API**: Drop-in replacement
- âœ… **Autoscaling**: HPA on GKE with L4/A100 GPUs
- âœ… **Cost Tracking**: Per-request cost monitoring

### Evaluation
- âœ… **Automated Metrics**: ROUGE, BLEU, perplexity
- âœ… **A/B Testing**: Compare model versions
- âœ… **Custom Benchmarks**: Domain-specific evaluation

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          LLMForge Platform                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Frontend  â”‚â”€â”€â”€â”€â–¶â”‚   Backend   â”‚â”€â”€â”€â”€â–¶â”‚   Vertex AI         â”‚   â”‚
â”‚  â”‚  (Next.js)  â”‚     â”‚  (FastAPI)  â”‚     â”‚   Training Jobs     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                   â”‚                       â”‚               â”‚
â”‚         â”‚                   â–¼                       â–¼               â”‚
â”‚         â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚         â”‚            â”‚  PostgreSQL â”‚         â”‚    GCS    â”‚         â”‚
â”‚         â”‚            â”‚  (Jobs DB)  â”‚         â”‚  (Models) â”‚         â”‚
â”‚         â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                   â”‚                       â”‚               â”‚
â”‚         â–¼                   â–¼                       â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    GKE Cluster (GPU Pool)                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  vLLM   â”‚  â”‚  vLLM   â”‚  â”‚  vLLM   â”‚  â”‚  Prometheus   â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Pod 1  â”‚  â”‚  Pod 2  â”‚  â”‚  Pod N  â”‚  â”‚  + Grafana    â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11
- Docker & Docker Compose
- GCP Account (for full deployment)
- NVIDIA GPU with CUDA 12.1+ (for training)

### Local Development

1. **Clone the repository**
   ```bash
   cd llmforge
   ```

2. **Start local services**
   ```bash
   docker-compose up -d
   ```
   
   This starts:
   - PostgreSQL (port 5432)
   - MLflow (port 5000)
   - Backend API (port 8000)
   - Frontend (port 3000)

3. **Access the dashboard**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000/docs
   - MLflow: http://localhost:5000

### Training a Model

1. **Prepare your dataset** (JSONL format):
   ```json
   {"instruction": "What is machine learning?", "input": "", "output": "Machine learning is..."}
   {"instruction": "Explain neural networks", "input": "", "output": "Neural networks are..."}
   ```

2. **Upload to GCS**:
   ```bash
   gsutil cp dataset.jsonl gs://your-bucket/data/
   ```

3. **Run training**:
   ```bash
   cd training
   pip install -r requirements.txt
   
   python train.py \
     gs://your-bucket/data/dataset.jsonl \
     ./outputs/my-model \
     --base-model meta-llama/Llama-3.1-8B \
     --num-epochs 3
   ```

4. **Evaluate the model**:
   ```bash
   python evaluate.py \
     meta-llama/Llama-3.1-8B \
     ./outputs/my-model \
     gs://your-bucket/data/eval.jsonl
   ```

## ğŸ“ Project Structure

```
llmforge/
â”œâ”€â”€ .env.example              # Environment configuration template
â”œâ”€â”€ .gitignore               # Git ignore rules
â”œâ”€â”€ docker-compose.yml       # Local development stack
â”œâ”€â”€ README.md                # This file
â”‚
â”œâ”€â”€ training/                # Training module
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile          # Training container
â”‚   â”œâ”€â”€ config.py           # Hyperparameter configuration
â”‚   â”œâ”€â”€ data_prep.py        # Dataset loading & formatting
â”‚   â”œâ”€â”€ train.py            # Main QLoRA training script
â”‚   â”œâ”€â”€ evaluate.py         # Model evaluation suite
â”‚   â”œâ”€â”€ merge_and_quantize.py # Post-training optimization
â”‚   â””â”€â”€ scripts/            # Shell scripts
â”‚       â”œâ”€â”€ train_llama_8b.sh
â”‚       â”œâ”€â”€ train_mistral_7b.sh
â”‚       â””â”€â”€ evaluate_model.sh
â”‚
â”œâ”€â”€ deployment/             # Deployment module
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile.vllm     # vLLM inference container
â”‚   â”œâ”€â”€ serve.py            # FastAPI wrapper for vLLM
â”‚   â”œâ”€â”€ k8s/                # Kubernetes manifests
â”‚   â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â”‚   â””â”€â”€ ingress.yaml
â”‚   â””â”€â”€ terraform/          # GKE infrastructure
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ variables.tf
â”‚       â””â”€â”€ outputs.tf
â”‚
â”œâ”€â”€ backend/                # Job orchestration API
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main.py            # FastAPI application
â”‚   â”œâ”€â”€ models.py          # Pydantic models
â”‚   â”œâ”€â”€ database.py        # SQLAlchemy setup
â”‚   â”œâ”€â”€ routers/           # API endpoints
â”‚   â”‚   â”œâ”€â”€ jobs.py
â”‚   â”‚   â”œâ”€â”€ deployments.py
â”‚   â”‚   â””â”€â”€ inference.py
â”‚   â””â”€â”€ services/          # Business logic
â”‚       â”œâ”€â”€ vertex_ai.py
â”‚       â”œâ”€â”€ mlflow_client.py
â”‚       â””â”€â”€ cost_calculator.py
â”‚
â”œâ”€â”€ frontend/              # Next.js dashboard
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ next.config.ts
â”‚   â”œâ”€â”€ tailwind.config.ts
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ app/           # Pages
â”‚       â”œâ”€â”€ components/    # React components
â”‚       â””â”€â”€ lib/           # Utilities
â”‚
â””â”€â”€ notebooks/             # Jupyter notebooks
    â”œâ”€â”€ 01_dataset_exploration.ipynb
    â”œâ”€â”€ 02_training_llama3.ipynb
    â””â”€â”€ 03_model_evaluation.ipynb
```

## âš™ï¸ Configuration

### Environment Variables

Copy `.env.example` to `.env` and configure:

```bash
# GCP
GCP_PROJECT_ID=your-project
GCP_REGION=us-central1
GCS_BUCKET=your-bucket

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/llmforge

# MLflow
MLFLOW_TRACKING_URI=http://localhost:5000

# Hugging Face (for gated models)
HF_TOKEN=hf_xxxx
```

### Training Hyperparameters

Default QLoRA configuration (optimized for Llama 3.1 8B on A100 40GB):

| Parameter | Default | Description |
|-----------|---------|-------------|
| `lora_r` | 64 | LoRA rank (8-128) |
| `lora_alpha` | 16 | LoRA scaling factor |
| `lora_dropout` | 0.05 | Dropout rate |
| `learning_rate` | 2e-4 | Learning rate |
| `num_epochs` | 3 | Training epochs |
| `batch_size` | 4 | Per-device batch size |
| `max_seq_length` | 2048 | Context window |

## ğŸ”§ GCP Deployment

### 1. Create GKE Cluster

```bash
cd deployment/terraform

# Initialize Terraform
terraform init

# Plan deployment
terraform plan -var="project_id=YOUR_PROJECT"

# Apply
terraform apply -var="project_id=YOUR_PROJECT"
```

### 2. Deploy vLLM

```bash
# Get cluster credentials
gcloud container clusters get-credentials llmforge-cluster --region us-central1

# Apply Kubernetes manifests
kubectl apply -f deployment/k8s/namespace.yaml
kubectl apply -f deployment/k8s/deployment.yaml
kubectl apply -f deployment/k8s/service.yaml
kubectl apply -f deployment/k8s/hpa.yaml
```

### 3. Configure DNS

Point your domain to the LoadBalancer IP and apply ingress:
```bash
kubectl apply -f deployment/k8s/ingress.yaml
```

## ğŸ’° Cost Estimates

### Training Costs (GCP)

| Model | GPU | Time (10K samples, 3 epochs) | Cost |
|-------|-----|------------------------------|------|
| Llama 3.1 8B | A100-40GB | ~4 hours | ~$15 |
| Llama 3.1 70B | A100-80GB | ~20 hours | ~$105 |
| Mistral 7B | A100-40GB | ~3.5 hours | ~$13 |

### Inference Costs

| GPU | Cost/Hour | Throughput | Cost/1K tokens |
|-----|-----------|------------|----------------|
| L4 | $1.12 | ~500 req/hr | ~$0.002 |
| A100-40GB | $3.67 | ~1000 req/hr | ~$0.006 |

## ğŸ“Š API Reference

### Create Training Job

```bash
curl -X POST http://localhost:8000/api/v1/jobs \
  -H "Content-Type: application/json" \
  -d '{
    "job_name": "my-llama-finetune",
    "base_model": "meta-llama/Llama-3.1-8B",
    "dataset_path": "gs://my-bucket/data/train.jsonl",
    "hyperparameters": {
      "lora_r": 64,
      "lora_alpha": 16,
      "learning_rate": 0.0002,
      "num_epochs": 3
    },
    "gpu_type": "A100-40GB"
  }'
```

### Inference (OpenAI-compatible)

```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is machine learning?",
    "max_tokens": 256,
    "temperature": 0.7
  }'
```

## ğŸ”¬ Evaluation Metrics

LLMForge evaluates models with:

- **ROUGE-L**: Measures longest common subsequence
- **BLEU**: Measures n-gram overlap
- **Perplexity**: Measures model confidence
- **Exact Match**: Measures exact answer accuracy

Target metrics for quality fine-tuning:
- ROUGE-L > 0.5
- BLEU > 0.35
- Perplexity improvement > 20%

## ğŸ› ï¸ Development

### Backend

```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```

### Frontend

```bash
cd frontend
npm install
npm run dev
```

### Training (local)

```bash
cd training
pip install -r requirements.txt
python train.py sample_data.jsonl ./output
```

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- [Hugging Face](https://huggingface.co/) - Transformers, PEFT, TRL
- [vLLM](https://vllm.ai/) - High-throughput inference
- [QLoRA](https://arxiv.org/abs/2305.14314) - Efficient fine-tuning
- [Unsloth](https://unsloth.ai/) - Training optimizations
